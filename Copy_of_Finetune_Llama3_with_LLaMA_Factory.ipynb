{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samzong/oversea-docs/blob/main/Copy_of_Finetune_Llama3_with_LLaMA_Factory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efb1390-77fa-403d-c377-30e1d22689ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 11951, done.\u001b[K\n",
            "remote: Counting objects: 100% (742/742), done.\u001b[K\n",
            "remote: Compressing objects: 100% (329/329), done.\u001b[K\n",
            "remote: Total 11951 (delta 464), reused 621 (delta 399), pack-reused 11209\u001b[K\n",
            "Receiving objects: 100% (11951/11951), 218.08 MiB | 16.05 MiB/s, done.\n",
            "Resolving deltas: 100% (8698/8698), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       docker-compose.yml  \u001b[01;34mexamples\u001b[0m/  pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  Dockerfile          LICENSE    README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mevaluation\u001b[0m/         Makefile   README_zh.md    setup.py\n",
            "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-edpoun7s/unsloth_0db0e01ea3154af2b9146f4a474d3e3a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-edpoun7s/unsloth_0db0e01ea3154af2b9146f4a474d3e3a\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 2f2b478868f63b66aaaa93db66ab3d811cddc95e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n",
            "Collecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.5-py3-none-any.whl size=105132 sha256=c862137a981db4b2436f81c114c30210126fd658919f75b83ca8cbb0cae0bb58\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hweqcyrf/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: xxhash, unsloth, shtab, dill, multiprocess, huggingface-hub, tyro, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 shtab-1.7.1 tyro-0.8.4 unsloth-2024.5 xxhash-3.4.1\n",
            "Collecting xformers==0.0.25\n",
            "  Downloading xformers-0.0.25-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.25\n",
            "Processing /content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (4.40.2)\n",
            "Requirement already satisfied: datasets>=2.14.3 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.19.1)\n",
            "Collecting accelerate>=0.27.2 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.10.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.1 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio>=4.0.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading gradio-4.31.4-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.7.2.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.7.2.dev0)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.7.1)\n",
            "Collecting fastapi (from llamafactory==0.7.2.dev0)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from llamafactory==0.7.2.dev0)\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.7.2.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.7.2.dev0) (2.2.1+cu121)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.7.2.dev0)\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->llamafactory==0.7.2.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.2.2)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.16.4 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading gradio_client-0.16.4-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading ruff-0.4.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.4->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.7.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.7.2.dev0) (2.18.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.7.2.dev0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.7.2.dev0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->llamafactory==0.7.2.dev0) (0.19.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl>=0.8.1->llamafactory==0.7.2.dev0) (0.8.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.7.2.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.7.2.dev0) (2.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.7.2.dev0) (3.7.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.12.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.7.2.dev0) (3.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory==0.7.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.7.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->llamafactory==0.7.2.dev0) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.14.3->llamafactory==0.7.2.dev0) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.7.2.dev0) (1.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory==0.7.2.dev0) (1.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->llamafactory==0.7.2.dev0)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.7.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.18.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.7.2.dev0) (0.1.2)\n",
            "Building wheels for collected packages: fire, llamafactory, ffmpy\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=fa7ea2b645feb3bd1dd8663817deeb245b4dd06d4ebf966f9da0cb3e3315cc36\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building wheel for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.7.2.dev0-py3-none-any.whl size=170181 sha256=f9781f0fa7b419fa77d46c7c96825b51be48aacc6b3bb593fd4ea63d6aa9ed58\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=f881b632252892808763acd2cf125fee0d0c3d83d1d3d2663379963393dbe077\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built fire llamafactory ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, aiofiles, watchfiles, uvicorn, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, typer, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, fastapi-cli, fastapi, bitsandbytes, accelerate, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 aiofiles-23.2.1 bitsandbytes-0.43.1 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.31.4 gradio-client-0.16.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.7.2.dev0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 orjson-3.10.3 peft-0.11.1 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.4 semantic-version-2.10.0 shellingham-1.5.4 sse-starlette-2.1.0 starlette-0.37.2 tomlkit-0.12.0 trl-0.8.6 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers==0.0.25\n",
        "!pip install .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Identity Dataset"
      ],
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb4e859-a700-438d-91b3-1d2f811d6fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185bf018-b781-400e-da9e-3555596e2518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-05-20 01:00:05.901060: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-20 01:00:05.901114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-20 01:00:06.013028: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-20 01:00:07.564625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://c3eb4ce15ca1332ac2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-05-20 01:05:23.539390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-20 01:05:23.539450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-20 01:05:23.540789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-20 01:05:24.822733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/20/2024 01:05:28 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "05/20/2024 01:05:28 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 3.17k/3.17k [00:00<00:00, 20.8MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 1.60MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 7.94MB/s]\n",
            "added_tokens.json: 100% 293/293 [00:00<00:00, 2.06MB/s]\n",
            "special_tokens_map.json: 100% 568/568 [00:00<00:00, 3.95MB/s]\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-20 01:05:30,128 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-20 01:05:30,128 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-20 01:05:30,128 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-20 01:05:30,128 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-20 01:05:30,128 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-20 01:05:30,219 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "05/20/2024 01:05:30 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "05/20/2024 01:05:30 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "05/20/2024 01:05:30 - INFO - llamafactory.data.loader - Loading dataset alpaca_zh_demo.json...\n",
            "Generating train split: 1000 examples [00:00, 42230.63 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1696.25 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:04<00:00, 244.54 examples/s]\n",
            "input_ids:\n",
            "[1, 32006, 887, 526, 263, 8444, 319, 29902, 20255, 29889, 32007, 29871, 13, 32010, 29871, 235, 178, 137, 232, 139, 174, 31666, 31201, 236, 138, 141, 31999, 30495, 31025, 30746, 30275, 30210, 31977, 30502, 31030, 30415, 30687, 235, 177, 189, 30383, 234, 190, 137, 235, 134, 161, 30687, 235, 177, 189, 30503, 30325, 30869, 31639, 30267, 32007, 29871, 13, 32001, 29871, 234, 190, 137, 235, 134, 161, 30687, 235, 177, 189, 30392, 30486, 30834, 31030, 30415, 30210, 30287, 30502, 30687, 235, 177, 189, 30214, 232, 177, 134, 31439, 30573, 30744, 30417, 30486, 31237, 30988, 30769, 30392, 31272, 31935, 30446, 30210, 31359, 30346, 31166, 30824, 30003, 30003, 234, 190, 137, 235, 134, 161, 30744, 31901, 30494, 30267, 30810, 30392, 30486, 30834, 30415, 30210, 30287, 30502, 31359, 234, 164, 131, 30687, 235, 177, 189, 30214, 31439, 30573, 234, 190, 137, 235, 134, 161, 30392, 30744, 30417, 30486, 30834, 30210, 31359, 30346, 31320, 31901, 30503, 31134, 30815, 31166, 30956, 30214, 30744, 30417, 30210, 30486, 30834, 30769, 30392, 31272, 30287, 30502, 31391, 30923, 30502, 234, 190, 137, 235, 134, 161, 31263, 30494, 30214, 234, 190, 137, 235, 134, 161, 31557, 30815, 30768, 31138, 234, 190, 137, 235, 134, 161, 30748, 235, 166, 133, 231, 189, 170, 30486, 30374, 30210, 234, 190, 137, 235, 134, 161, 30267, 30810, 30287, 30687, 235, 177, 189, 31272, 235, 153, 158, 30495, 235, 179, 151, 30330, 233, 153, 192, 234, 150, 169, 30728, 30503, 31731, 235, 145, 180, 30909, 29896, 29947, 29941, 29929, 30470, 31688, 30936, 31302, 30544, 30267, 13, 13, 30325, 30869, 31639, 30392, 31084, 30654, 31430, 30392, 30654, 31430, 31185, 30210, 30275, 30869, 30214, 30953, 31238, 30392, 31639, 30214, 30448, 30900, 232, 158, 183, 234, 190, 152, 30654, 31430, 233, 154, 142, 31415, 30210, 30687, 235, 177, 189, 30267, 30810, 30502, 30687, 235, 177, 189, 31656, 234, 163, 183, 30743, 31471, 31675, 30210, 30533, 30869, 31639, 235, 170, 133, 30940, 30214, 31439, 30573, 30533, 31539, 31666, 30413, 30392, 31503, 232, 177, 156, 30210, 30275, 30869, 30267, 30325, 30869, 31639, 30210, 31302, 30544, 30767, 30392, 232, 179, 191, 30960, 31543, 30064, 232, 150, 168, 30868, 232, 179, 191, 30214, 31221, 30505, 29896, 29953, 30793, 234, 189, 173, 31120, 30910, 30746, 30743, 31221, 30210, 235, 148, 154, 30732, 30866, 30408, 30988, 31894, 30448, 235, 177, 189, 30843, 30214, 236, 155, 147, 235, 194, 179, 30743, 30654, 31430, 31185, 30448, 30900, 232, 158, 183, 234, 190, 152, 30654, 31430, 31894, 30448, 30210, 31382, 30883, 30214, 30573, 30408, 30333, 30415, 30210, 30910, 31599, 232, 132, 157, 30544, 30743, 232, 186, 171, 30257, 235, 183, 164, 234, 143, 177, 30267, 32007]\n",
            "inputs:\n",
            "<s><|system|> You are a helpful AI assistant.<|end|> \n",
            "<|user|> 识别并解释给定列表中的两个科学理论：细胞理论和日心说。<|end|> \n",
            "<|assistant|> 细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
            "\n",
            "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 234, 190, 137, 235, 134, 161, 30687, 235, 177, 189, 30392, 30486, 30834, 31030, 30415, 30210, 30287, 30502, 30687, 235, 177, 189, 30214, 232, 177, 134, 31439, 30573, 30744, 30417, 30486, 31237, 30988, 30769, 30392, 31272, 31935, 30446, 30210, 31359, 30346, 31166, 30824, 30003, 30003, 234, 190, 137, 235, 134, 161, 30744, 31901, 30494, 30267, 30810, 30392, 30486, 30834, 30415, 30210, 30287, 30502, 31359, 234, 164, 131, 30687, 235, 177, 189, 30214, 31439, 30573, 234, 190, 137, 235, 134, 161, 30392, 30744, 30417, 30486, 30834, 30210, 31359, 30346, 31320, 31901, 30503, 31134, 30815, 31166, 30956, 30214, 30744, 30417, 30210, 30486, 30834, 30769, 30392, 31272, 30287, 30502, 31391, 30923, 30502, 234, 190, 137, 235, 134, 161, 31263, 30494, 30214, 234, 190, 137, 235, 134, 161, 31557, 30815, 30768, 31138, 234, 190, 137, 235, 134, 161, 30748, 235, 166, 133, 231, 189, 170, 30486, 30374, 30210, 234, 190, 137, 235, 134, 161, 30267, 30810, 30287, 30687, 235, 177, 189, 31272, 235, 153, 158, 30495, 235, 179, 151, 30330, 233, 153, 192, 234, 150, 169, 30728, 30503, 31731, 235, 145, 180, 30909, 29896, 29947, 29941, 29929, 30470, 31688, 30936, 31302, 30544, 30267, 13, 13, 30325, 30869, 31639, 30392, 31084, 30654, 31430, 30392, 30654, 31430, 31185, 30210, 30275, 30869, 30214, 30953, 31238, 30392, 31639, 30214, 30448, 30900, 232, 158, 183, 234, 190, 152, 30654, 31430, 233, 154, 142, 31415, 30210, 30687, 235, 177, 189, 30267, 30810, 30502, 30687, 235, 177, 189, 31656, 234, 163, 183, 30743, 31471, 31675, 30210, 30533, 30869, 31639, 235, 170, 133, 30940, 30214, 31439, 30573, 30533, 31539, 31666, 30413, 30392, 31503, 232, 177, 156, 30210, 30275, 30869, 30267, 30325, 30869, 31639, 30210, 31302, 30544, 30767, 30392, 232, 179, 191, 30960, 31543, 30064, 232, 150, 168, 30868, 232, 179, 191, 30214, 31221, 30505, 29896, 29953, 30793, 234, 189, 173, 31120, 30910, 30746, 30743, 31221, 30210, 235, 148, 154, 30732, 30866, 30408, 30988, 31894, 30448, 235, 177, 189, 30843, 30214, 236, 155, 147, 235, 194, 179, 30743, 30654, 31430, 31185, 30448, 30900, 232, 158, 183, 234, 190, 152, 30654, 31430, 31894, 30448, 30210, 31382, 30883, 30214, 30573, 30408, 30333, 30415, 30210, 30910, 31599, 232, 132, 157, 30544, 30743, 232, 186, 171, 30257, 235, 183, 164, 234, 143, 177, 30267, 32007]\n",
            "labels:\n",
            "细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施瓦内和雪莱于1839年首次提出。\n",
            "\n",
            "日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳运行的模型，为天文学的发展做出了巨大贡献。<|end|>\n",
            "config.json: 100% 904/904 [00:00<00:00, 4.92MB/s]\n",
            "[INFO|configuration_utils.py:726] 2024-05-20 01:05:36,447 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/config.json\n",
            "configuration_phi3.py: 100% 10.4k/10.4k [00:00<00:00, 38.7MB/s]\n",
            "[INFO|configuration_utils.py:726] 2024-05-20 01:05:36,796 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-20 01:05:36,798 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "05/20/2024 01:05:36 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.\n",
            "modeling_phi3.py: 100% 73.8k/73.8k [00:00<00:00, 104MB/s]\n",
            "model.safetensors.index.json: 100% 16.3k/16.3k [00:00<00:00, 59.9MB/s]\n",
            "[INFO|modeling_utils.py:3429] 2024-05-20 01:05:37,461 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:00<02:42, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.97G [00:00<01:53, 43.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<01:41, 48.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:00<01:33, 52.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:01<01:29, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:01<01:26, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/4.97G [00:01<01:24, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:01<01:23, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:01<01:22, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:01<01:21, 59.6MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.97G [00:02<01:21, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.97G [00:02<01:20, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:02<01:20, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:02<01:20, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.97G [00:02<01:20, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:02<01:20, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.97G [00:03<01:20, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:03<01:20, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.97G [00:03<01:20, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:03<01:20, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:03<01:19, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:04<01:19, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.97G [00:04<01:18, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:04<01:18, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.97G [00:04<01:18, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:04<01:18, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:04<01:18, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.97G [00:05<01:17, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.97G [00:05<01:17, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:05<01:18, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.97G [00:05<01:18, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.97G [00:05<01:18, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:05<01:18, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:06<01:19, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.97G [00:06<01:18, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:06<01:17, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.97G [00:06<01:16, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:06<01:16, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:07<01:16, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:07<01:16, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.97G [00:07<01:15, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.97G [00:07<01:16, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:07<01:16, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:07<01:16, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.97G [00:08<01:18, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:08<01:17, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:08<01:17, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.97G [00:08<01:18, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:08<01:17, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.97G [00:09<01:18, 56.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:09<01:17, 57.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:09<01:18, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.97G [00:09<01:19, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:09<01:19, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:09<01:19, 55.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.97G [00:10<01:20, 54.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.97G [00:10<01:19, 54.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.97G [00:10<01:19, 54.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:10<01:18, 55.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:10<01:18, 55.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.97G [00:11<01:16, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:11<01:15, 57.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:11<01:53, 38.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.97G [00:11<01:40, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:12<01:31, 46.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.97G [00:12<01:25, 49.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:12<01:21, 52.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:12<01:17, 54.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.97G [00:12<01:15, 56.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.97G [00:12<01:14, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:13<01:14, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.97G [00:13<01:13, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.97G [00:13<01:12, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:13<01:12, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:13<01:12, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:14<01:11, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:14<01:10, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.97G [00:14<01:09, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:14<01:09, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:14<01:09, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:14<01:08, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.97G [00:15<01:08, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:15<01:08, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.97G [00:15<01:08, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.97G [00:15<01:08, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:15<01:08, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:15<01:08, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.97G [00:16<01:08, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:16<01:09, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/4.97G [00:16<01:09, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:16<01:09, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:16<01:07, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.97G [00:17<01:07, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:17<01:06, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:17<01:06, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:17<01:06, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.97G [00:17<01:06, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:17<01:05, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:18<01:05, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.97G [00:18<01:05, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:18<01:05, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.97G [00:18<01:05, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.97G [00:18<01:07, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:19<01:07, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.97G [00:19<01:06, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:19<01:07, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:19<01:08, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:19<01:07, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:19<01:06, 57.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.97G [00:20<01:06, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:20<01:06, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:20<01:05, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:20<01:04, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.97G [00:20<01:04, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:21<01:05, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:21<01:04, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:21<01:04, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:21<01:03, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:21<01:02, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.97G [00:21<01:02, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:22<01:02, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:22<01:01, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.97G [00:22<01:03, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:22<01:03, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [00:22<01:03, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:23<01:04, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:23<01:05, 56.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:23<01:05, 55.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:23<01:05, 55.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:23<01:06, 54.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:23<01:07, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:24<01:06, 53.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:24<01:06, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:24<01:06, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.97G [00:24<01:06, 53.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:24<01:03, 55.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:25<01:02, 56.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.97G [00:25<01:02, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:25<01:01, 57.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:25<00:59, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:25<00:59, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:26<01:31, 38.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.97G [00:26<01:20, 43.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.97G [00:26<01:13, 47.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:26<01:08, 50.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:27<01:05, 52.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.97G [00:27<01:03, 54.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:27<01:01, 55.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.97G [00:27<01:00, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:27<01:01, 55.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:27<00:59, 56.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:28<00:58, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.97G [00:28<00:57, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:28<00:56, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:28<00:56, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:28<00:56, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:28<00:55, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:29<00:55, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:29<00:55, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:29<00:54, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.97G [00:29<00:56, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.97G [00:29<00:55, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:30<00:55, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:30<00:54, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.97G [00:30<00:54, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:30<00:53, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [00:30<00:53, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.97G [00:30<00:53, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:31<00:53, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.97G [00:31<00:53, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:31<00:53, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.97G [00:31<00:53, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.97G [00:31<00:54, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.97G [00:31<00:53, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:32<00:53, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [00:32<00:54, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [00:32<00:54, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.97G [00:32<00:53, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:32<00:52, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.97G [00:33<00:52, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:33<00:51, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.97G [00:33<00:51, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [00:33<00:51, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:33<00:50, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.97G [00:33<00:50, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:34<00:50, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:34<00:50, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [00:34<00:50, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:34<00:51, 57.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.97G [00:34<00:52, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.97G [00:35<00:53, 55.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [00:35<00:53, 55.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.97G [00:35<00:55, 53.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [00:35<00:53, 55.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.97G [00:35<00:51, 56.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:35<00:50, 57.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.97G [00:36<00:50, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [00:36<00:49, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:36<00:49, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [00:36<00:50, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.97G [00:36<00:50, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:37<00:51, 55.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.97G [00:37<00:51, 55.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:37<00:50, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:37<00:49, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.97G [00:37<00:49, 57.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.97G [00:38<00:48, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:38<00:47, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.97G [00:38<00:47, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.97G [00:38<00:47, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [00:38<00:48, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [00:38<00:47, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [00:39<00:48, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:39<00:47, 57.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.97G [00:39<00:47, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.97G [00:39<00:46, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:39<00:46, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.97G [00:39<00:45, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.97G [00:40<00:45, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:40<00:44, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.97G [00:40<00:44, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.97G [00:40<00:44, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:40<00:44, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [00:41<00:43, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.97G [00:41<00:43, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [00:41<00:43, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:41<00:43, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.97G [00:41<00:44, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:41<00:44, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.97G [00:42<00:44, 57.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.97G [00:42<00:44, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:42<00:43, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [00:42<00:42, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [00:42<00:42, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:43<00:42, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [00:43<00:41, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [00:43<00:41, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:43<00:41, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:43<00:41, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.97G [00:43<00:41, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.97G [00:44<00:40, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [00:44<00:40, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [00:44<00:40, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.97G [00:44<00:40, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.97G [00:44<00:40, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.97G [00:44<00:40, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:45<00:40, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.97G [00:45<00:39, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [00:45<00:39, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:45<00:39, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [00:45<00:39, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.97G [00:45<00:38, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:46<00:38, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.97G [00:46<00:38, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.97G [00:46<00:38, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.97G [00:46<00:38, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:46<00:38, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.97G [00:47<00:58, 38.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [00:47<00:53, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:47<00:49, 45.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [00:47<00:47, 47.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [00:48<00:45, 49.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.97G [00:48<00:43, 50.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.97G [00:48<00:42, 51.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.97G [00:48<00:41, 52.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:48<00:41, 53.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.97G [00:49<00:40, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.97G [00:49<00:39, 54.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [00:49<00:38, 55.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.97G [00:49<00:37, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [00:49<00:37, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.97G [00:50<00:36, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [00:50<00:36, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:50<00:35, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:50<00:35, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.97G [00:50<00:34, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.97G [00:50<00:34, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [00:51<00:34, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:51<00:34, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:51<00:35, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.97G [00:51<00:34, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.97G [00:51<00:34, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.97G [00:51<00:34, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:52<00:33, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.97G [00:52<00:33, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.97G [00:52<00:33, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:52<00:32, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:52<00:32, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [00:53<00:32, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [00:53<00:32, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.97G [00:53<00:32, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.97G [00:53<00:31, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:53<00:31, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.97G [00:53<00:31, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.97G [00:54<00:31, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [00:54<00:31, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [00:54<00:31, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.97G [00:54<00:30, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:54<00:30, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [00:54<00:30, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.97G [00:55<00:30, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [00:55<00:30, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:55<00:29, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.97G [00:55<00:29, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [00:55<00:30, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:55<00:30, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.97G [00:56<00:30, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [00:56<00:30, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:56<00:29, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.97G [00:56<00:29, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:56<00:29, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:57<00:28, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.97G [00:57<00:28, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [00:57<00:28, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.97G [00:57<00:27, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [00:57<00:27, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:57<00:27, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.97G [00:58<00:27, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [00:58<00:27, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.97G [00:58<00:27, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.97G [00:58<00:27, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.97G [00:58<00:27, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:59<00:27, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.97G [00:59<00:27, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.41G/4.97G [00:59<00:27, 57.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:59<00:26, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.97G [00:59<00:26, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.97G [00:59<00:25, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [01:00<00:25, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [01:00<00:25, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.97G [01:00<00:25, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [01:00<00:25, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [01:00<00:25, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.97G [01:00<00:24, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [01:01<00:25, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [01:01<00:25, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.97G [01:01<00:25, 55.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [01:01<00:25, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.97G [01:01<00:25, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.97G [01:02<00:25, 54.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [01:02<00:24, 56.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.97G [01:02<00:24, 56.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [01:02<00:23, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [01:02<00:23, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [01:03<00:23, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [01:03<00:22, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.97G [01:03<00:22, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [01:03<00:22, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.97G [01:03<00:21, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [01:03<00:21, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [01:04<00:21, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.97G [01:04<00:22, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.97G [01:04<00:22, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [01:04<00:21, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.97G [01:04<00:21, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [01:05<00:32, 38.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [01:05<00:29, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.97G [01:05<00:26, 46.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.97G [01:05<00:24, 49.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [01:06<00:22, 52.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [01:06<00:21, 54.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.97G [01:06<00:21, 55.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [01:06<00:20, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.97G [01:06<00:20, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.97G [01:06<00:19, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [01:07<00:20, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.97G [01:07<00:19, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.97G [01:07<00:19, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [01:07<00:18, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.97G [01:07<00:18, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [01:07<00:18, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.97G [01:08<00:18, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.97G [01:08<00:17, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.97G [01:08<00:17, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [01:08<00:17, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [01:08<00:17, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [01:09<00:17, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.97G [01:09<00:16, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [01:09<00:16, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.97G [01:09<00:16, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [01:09<00:16, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.97G [01:09<00:16, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.97G [01:10<00:16, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [01:10<00:15, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [01:10<00:15, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.97G [01:10<00:15, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [01:10<00:15, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [01:10<00:15, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [01:11<00:14, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.97G [01:11<00:15, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [01:11<00:14, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.97G [01:11<00:15, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [01:11<00:14, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [01:12<00:14, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [01:12<00:14, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.97G [01:12<00:13, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [01:12<00:13, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.97G [01:12<00:13, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [01:12<00:13, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [01:13<00:13, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.97G [01:13<00:13, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.97G [01:13<00:13, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [01:13<00:12, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [01:13<00:12, 56.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [01:14<00:12, 56.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.97G [01:14<00:12, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [01:14<00:12, 54.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [01:14<00:12, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [01:14<00:11, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/4.97G [01:15<00:11, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [01:15<00:11, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.97G [01:15<00:11, 57.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [01:15<00:11, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [01:15<00:10, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.97G [01:15<00:10, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [01:16<00:10, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.97G [01:16<00:10, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [01:16<00:09, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [01:16<00:09, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [01:16<00:09, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.97G [01:16<00:09, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [01:17<00:09, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [01:17<00:09, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [01:17<00:08, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.97G [01:17<00:08, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [01:17<00:08, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.97G [01:18<00:08, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.97G [01:18<00:08, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [01:18<00:07, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [01:18<00:07, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.97G [01:18<00:07, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [01:18<00:07, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.97G [01:19<00:07, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [01:19<00:07, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [01:19<00:07, 58.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.97G [01:19<00:06, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.97G [01:19<00:06, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [01:19<00:06, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.97G [01:20<00:06, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [01:20<00:06, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.97G [01:20<00:05, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.97G [01:20<00:05, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [01:20<00:05, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [01:20<00:05, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.97G [01:21<00:05, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [01:21<00:04, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [01:21<00:04, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.97G [01:21<00:04, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [01:21<00:04, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [01:22<00:04, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [01:22<00:04, 59.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.97G [01:22<00:03, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [01:22<00:03, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [01:22<00:03, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [01:22<00:03, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.97G [01:23<00:03, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.97G [01:23<00:03, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [01:23<00:02, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [01:23<00:02, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [01:23<00:02, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.97G [01:23<00:02, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.97G [01:24<00:02, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [01:24<00:01, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [01:24<00:01, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.97G [01:24<00:01, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.97G [01:24<00:01, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.97G [01:25<00:01, 59.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [01:25<00:01, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [01:25<00:00, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.97G [01:25<00:00, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.97G [01:25<00:00, 57.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.97G [01:25<00:00, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.96G/4.97G [01:26<00:00, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [01:26<00:00, 57.5MB/s]\n",
            "Downloading shards:  50% 1/2 [01:26<01:26, 86.58s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/2.67G [00:00<00:45, 58.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/2.67G [00:00<00:46, 57.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/2.67G [00:00<00:46, 56.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:45, 57.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 52.4M/2.67G [00:00<00:44, 58.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/2.67G [00:01<00:45, 57.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 73.4M/2.67G [00:01<00:44, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 83.9M/2.67G [00:01<01:10, 36.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:01<01:02, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 105M/2.67G [00:02<00:56, 45.5MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 115M/2.67G [00:02<00:52, 48.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 126M/2.67G [00:02<00:49, 51.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 136M/2.67G [00:02<00:47, 53.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:02<00:46, 54.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 157M/2.67G [00:03<00:45, 54.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 168M/2.67G [00:03<00:44, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 178M/2.67G [00:03<00:43, 57.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:03<00:42, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:03<00:42, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 210M/2.67G [00:03<00:41, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 220M/2.67G [00:04<00:41, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:04<00:40, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 241M/2.67G [00:04<00:40, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 252M/2.67G [00:04<00:40, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 262M/2.67G [00:04<00:40, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 273M/2.67G [00:04<00:40, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:05<00:40, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 294M/2.67G [00:05<00:40, 59.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:05<00:40, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 315M/2.67G [00:05<00:40, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 325M/2.67G [00:05<00:41, 57.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 336M/2.67G [00:06<00:40, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:06<00:39, 58.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 357M/2.67G [00:06<00:39, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 367M/2.67G [00:06<00:38, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 377M/2.67G [00:06<00:38, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 388M/2.67G [00:06<00:38, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:07<00:38, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 409M/2.67G [00:07<00:38, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 419M/2.67G [00:07<00:37, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 430M/2.67G [00:07<00:37, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:07<00:37, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 451M/2.67G [00:07<00:36, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:08<00:36, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 472M/2.67G [00:08<00:36, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 482M/2.67G [00:08<00:36, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 493M/2.67G [00:08<00:37, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:08<00:37, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 514M/2.67G [00:09<00:36, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 524M/2.67G [00:09<00:37, 57.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 535M/2.67G [00:09<00:36, 58.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:09<00:36, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 556M/2.67G [00:09<00:35, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:09<00:35, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 577M/2.67G [00:10<00:35, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 587M/2.67G [00:10<00:34, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 598M/2.67G [00:10<00:34, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 608M/2.67G [00:10<00:34, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:10<00:34, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 629M/2.67G [00:11<00:33, 60.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 640M/2.67G [00:11<00:34, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:11<00:34, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 661M/2.67G [00:11<00:34, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:11<00:34, 57.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 682M/2.67G [00:11<00:35, 56.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 692M/2.67G [00:12<00:35, 56.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 703M/2.67G [00:12<00:35, 55.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 713M/2.67G [00:12<00:35, 54.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 724M/2.67G [00:12<00:35, 55.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 734M/2.67G [00:12<00:34, 55.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 744M/2.67G [00:13<00:52, 37.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:13<00:46, 40.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 765M/2.67G [00:13<00:42, 44.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:13<00:40, 47.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 786M/2.67G [00:14<00:38, 48.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 797M/2.67G [00:14<00:37, 50.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 807M/2.67G [00:14<00:35, 52.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 818M/2.67G [00:14<00:33, 54.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:14<00:32, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 839M/2.67G [00:15<00:31, 57.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 849M/2.67G [00:15<00:31, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:15<00:30, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:15<00:30, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:15<00:30, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 891M/2.67G [00:15<00:29, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 902M/2.67G [00:16<00:29, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 912M/2.67G [00:16<00:30, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:16<00:29, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 933M/2.67G [00:16<00:29, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 944M/2.67G [00:16<00:29, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 954M/2.67G [00:17<00:29, 57.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:17<00:29, 58.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:17<00:28, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 986M/2.67G [00:17<00:28, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 996M/2.67G [00:17<00:28, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.01G/2.67G [00:17<00:27, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:18<00:27, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:18<00:27, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.04G/2.67G [00:18<00:27, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.05G/2.67G [00:18<00:27, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.06G/2.67G [00:18<00:26, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:18<00:26, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:19<00:26, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.09G/2.67G [00:19<00:26, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.10G/2.67G [00:19<00:26, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.11G/2.67G [00:19<00:26, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.12G/2.67G [00:19<00:25, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:19<00:25, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.14G/2.67G [00:20<00:25, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.15G/2.67G [00:20<00:25, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.16G/2.67G [00:20<00:25, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:20<00:24, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:20<00:24, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.20G/2.67G [00:21<00:24, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.21G/2.67G [00:21<00:24, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.22G/2.67G [00:21<00:24, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:21<00:24, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:21<00:24, 59.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.25G/2.67G [00:21<00:24, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.26G/2.67G [00:22<00:23, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:22<00:25, 53.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.28G/2.67G [00:22<00:37, 36.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:23<00:34, 40.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.30G/2.67G [00:23<00:45, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.31G/2.67G [00:23<00:38, 35.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.32G/2.67G [00:23<00:33, 40.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:24<00:30, 44.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:24<00:28, 47.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:24<00:26, 49.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.36G/2.67G [00:24<00:25, 50.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.37G/2.67G [00:24<00:25, 51.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:25<00:24, 51.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:25<00:23, 53.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.41G/2.67G [00:25<00:22, 55.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.42G/2.67G [00:25<00:22, 56.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.43G/2.67G [00:25<00:21, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [00:25<00:21, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:26<00:21, 56.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.46G/2.67G [00:26<00:21, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.47G/2.67G [00:26<00:20, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.48G/2.67G [00:26<00:20, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:26<00:20, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:27<00:19, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.51G/2.67G [00:27<00:19, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.52G/2.67G [00:27<00:19, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.53G/2.67G [00:27<00:19, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:27<00:18, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:27<00:18, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.56G/2.67G [00:28<00:18, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.57G/2.67G [00:28<00:18, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.58G/2.67G [00:28<00:18, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.59G/2.67G [00:28<00:17, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:28<00:17, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.61G/2.67G [00:28<00:17, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.63G/2.67G [00:29<00:17, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.64G/2.67G [00:29<00:17, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:29<00:17, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.66G/2.67G [00:29<00:17, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.67G/2.67G [00:29<00:17, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.68G/2.67G [00:30<00:16, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.69G/2.67G [00:30<00:16, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:30<00:16, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.71G/2.67G [00:30<00:16, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.72G/2.67G [00:30<00:15, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:30<00:15, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [00:31<00:15, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:31<00:15, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:31<00:15, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.77G/2.67G [00:31<00:15, 59.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.78G/2.67G [00:31<00:15, 59.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.79G/2.67G [00:32<00:15, 57.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.80G/2.67G [00:32<00:14, 57.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:32<00:14, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.82G/2.67G [00:32<00:14, 58.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.84G/2.67G [00:32<00:14, 58.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.85G/2.67G [00:32<00:14, 58.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.86G/2.67G [00:35<01:06, 12.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:35<00:50, 16.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.88G/2.67G [00:35<00:38, 20.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.89G/2.67G [00:35<00:30, 25.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:36<00:25, 30.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.91G/2.67G [00:36<00:21, 36.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [00:36<00:18, 41.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:36<00:16, 45.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.94G/2.67G [00:36<00:14, 49.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.95G/2.67G [00:36<00:13, 51.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:37<00:13, 53.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.97G/2.67G [00:37<00:12, 53.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.98G/2.67G [00:37<00:12, 54.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [00:37<00:12, 54.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.00G/2.67G [00:37<00:12, 53.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.01G/2.67G [00:38<00:12, 52.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:38<00:11, 54.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.03G/2.67G [00:38<00:11, 55.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.04G/2.67G [00:38<00:10, 56.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [00:38<00:10, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.07G/2.67G [00:38<00:10, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.08G/2.67G [00:39<00:10, 56.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:39<00:10, 56.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.10G/2.67G [00:39<00:10, 57.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.11G/2.67G [00:39<00:09, 57.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:39<00:09, 58.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.13G/2.67G [00:40<00:09, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.14G/2.67G [00:40<00:08, 59.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.15G/2.67G [00:40<00:08, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.16G/2.67G [00:40<00:08, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.17G/2.67G [00:40<00:08, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [00:40<00:08, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [00:41<00:08, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.20G/2.67G [00:41<00:07, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.21G/2.67G [00:41<00:07, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [00:41<00:07, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:41<00:07, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [00:42<00:07, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.25G/2.67G [00:42<00:06, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.26G/2.67G [00:42<00:06, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [00:42<00:06, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [00:42<00:06, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.30G/2.67G [00:42<00:06, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.31G/2.67G [00:43<00:06, 58.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [00:43<00:06, 57.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.33G/2.67G [00:43<00:05, 57.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.34G/2.67G [00:43<00:05, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [00:43<00:05, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.36G/2.67G [00:43<00:05, 57.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.37G/2.67G [00:44<00:05, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [00:44<00:04, 58.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.39G/2.67G [00:44<00:04, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.40G/2.67G [00:44<00:04, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.41G/2.67G [00:44<00:04, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.42G/2.67G [00:45<00:04, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:45<00:03, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.44G/2.67G [00:45<00:03, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.45G/2.67G [00:45<00:03, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.46G/2.67G [00:45<00:03, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.47G/2.67G [00:45<00:03, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.49G/2.67G [00:46<00:03, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [00:46<00:02, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.51G/2.67G [00:46<00:02, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:46<00:02, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.53G/2.67G [00:46<00:02, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [00:46<00:02, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.55G/2.67G [00:47<00:02, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.56G/2.67G [00:47<00:01, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [00:47<00:01, 57.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.58G/2.67G [00:47<00:01, 55.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [00:47<00:01, 54.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.60G/2.67G [00:48<00:01, 54.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.61G/2.67G [00:48<00:01, 54.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.62G/2.67G [00:48<00:00, 54.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.63G/2.67G [00:48<00:00, 55.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.64G/2.67G [00:48<00:00, 56.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.65G/2.67G [00:49<00:00, 37.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.66G/2.67G [00:49<00:00, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:49<00:00, 53.8MB/s]\n",
            "Downloading shards: 100% 2/2 [02:16<00:00, 68.21s/it]\n",
            "[INFO|modeling_utils.py:1494] 2024-05-20 01:07:53,878 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:928] 2024-05-20 01:07:53,881 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:34<00:00, 17.02s/it]\n",
            "[INFO|modeling_utils.py:4170] 2024-05-20 01:08:29,266 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4178] 2024-05-20 01:08:29,267 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 172/172 [00:00<00:00, 1.20MB/s]\n",
            "[INFO|configuration_utils.py:883] 2024-05-20 01:08:29,440 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/generation_config.json\n",
            "[INFO|configuration_utils.py:928] 2024-05-20 01:08:29,440 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "05/20/2024 01:08:29 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
            "05/20/2024 01:08:29 - INFO - llamafactory.model.utils.attention - Using vanilla attention implementation.\n",
            "05/20/2024 01:08:29 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "05/20/2024 01:08:29 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "05/20/2024 01:08:29 - INFO - llamafactory.model.loader - trainable params: 3145728 || all params: 3824225280 || trainable%: 0.0823\n",
            "[INFO|trainer.py:626] 2024-05-20 01:08:29,718 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2048] 2024-05-20 01:08:30,039 >> ***** Running training *****\n",
            "[INFO|trainer.py:2049] 2024-05-20 01:08:30,039 >>   Num examples = 1,000\n",
            "[INFO|trainer.py:2050] 2024-05-20 01:08:30,040 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2051] 2024-05-20 01:08:30,040 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2054] 2024-05-20 01:08:30,040 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2055] 2024-05-20 01:08:30,040 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2056] 2024-05-20 01:08:30,040 >>   Total optimization steps = 186\n",
            "[INFO|trainer.py:2057] 2024-05-20 01:08:30,041 >>   Number of trainable parameters = 3,145,728\n",
            "  3% 5/186 [00:58<34:17, 11.37s/it]05/20/2024 01:09:28 - INFO - llamafactory.extras.callbacks - {'loss': 1.0837, 'learning_rate': 4.9911e-05, 'epoch': 0.08}\n",
            "{'loss': 1.0837, 'grad_norm': 0.43542391061782837, 'learning_rate': 4.9910902453260824e-05, 'epoch': 0.08}\n",
            "  5% 10/186 [01:55<33:55, 11.56s/it]05/20/2024 01:10:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.1094, 'learning_rate': 4.9644e-05, 'epoch': 0.16}\n",
            "{'loss': 1.1094, 'grad_norm': 0.29286783933639526, 'learning_rate': 4.964424488287009e-05, 'epoch': 0.16}\n",
            "  8% 15/186 [02:56<35:07, 12.33s/it]05/20/2024 01:11:26 - INFO - llamafactory.extras.callbacks - {'loss': 1.1306, 'learning_rate': 4.9202e-05, 'epoch': 0.24}\n",
            "{'loss': 1.1306, 'grad_norm': 0.25051984190940857, 'learning_rate': 4.920192797165511e-05, 'epoch': 0.24}\n",
            " 11% 20/186 [04:01<35:40, 12.89s/it]05/20/2024 01:12:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.0209, 'learning_rate': 4.8587e-05, 'epoch': 0.32}\n",
            "{'loss': 1.0209, 'grad_norm': 0.34198975563049316, 'learning_rate': 4.858710446774951e-05, 'epoch': 0.32}\n",
            " 13% 25/186 [04:56<30:38, 11.42s/it]05/20/2024 01:13:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.9828, 'learning_rate': 4.7804e-05, 'epoch': 0.40}\n",
            "{'loss': 0.9828, 'grad_norm': 0.39222386479377747, 'learning_rate': 4.780415671242334e-05, 'epoch': 0.4}\n",
            " 16% 30/186 [06:06<35:50, 13.79s/it]05/20/2024 01:14:36 - INFO - llamafactory.extras.callbacks - {'loss': 1.0361, 'learning_rate': 4.6859e-05, 'epoch': 0.48}\n",
            "{'loss': 1.0361, 'grad_norm': 0.26393184065818787, 'learning_rate': 4.685866540361456e-05, 'epoch': 0.48}\n",
            " 19% 35/186 [07:13<33:42, 13.39s/it]05/20/2024 01:15:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.9211, 'learning_rate': 4.5757e-05, 'epoch': 0.56}\n",
            "{'loss': 0.9211, 'grad_norm': 0.29093796014785767, 'learning_rate': 4.5757369817809415e-05, 'epoch': 0.56}\n",
            " 22% 40/186 [08:18<31:08, 12.80s/it]05/20/2024 01:16:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.9668, 'learning_rate': 4.4508e-05, 'epoch': 0.64}\n",
            "{'loss': 0.9668, 'grad_norm': 0.40472257137298584, 'learning_rate': 4.45081197738023e-05, 'epoch': 0.64}\n",
            " 24% 45/186 [09:26<31:37, 13.46s/it]05/20/2024 01:17:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.9647, 'learning_rate': 4.3120e-05, 'epoch': 0.72}\n",
            "{'loss': 0.9647, 'grad_norm': 0.317975252866745, 'learning_rate': 4.3119819680728e-05, 'epoch': 0.72}\n",
            " 27% 50/186 [10:28<27:27, 12.12s/it]05/20/2024 01:18:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.9838, 'learning_rate': 4.1602e-05, 'epoch': 0.80}\n",
            "{'loss': 0.9838, 'grad_norm': 0.42926642298698425, 'learning_rate': 4.160236506918098e-05, 'epoch': 0.8}\n",
            " 30% 55/186 [11:34<28:00, 12.83s/it]05/20/2024 01:20:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.9912, 'learning_rate': 3.9967e-05, 'epoch': 0.88}\n",
            "{'loss': 0.9912, 'grad_norm': 0.26137709617614746, 'learning_rate': 3.9966572057815373e-05, 'epoch': 0.88}\n",
            " 32% 60/186 [12:37<26:25, 12.58s/it]05/20/2024 01:21:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.9798, 'learning_rate': 3.8224e-05, 'epoch': 0.96}\n",
            "{'loss': 0.9798, 'grad_norm': 0.33404481410980225, 'learning_rate': 3.822410025817406e-05, 'epoch': 0.96}\n",
            " 35% 65/186 [13:43<26:31, 13.16s/it]05/20/2024 01:22:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.8980, 'learning_rate': 3.6387e-05, 'epoch': 1.04}\n",
            "{'loss': 0.898, 'grad_norm': 0.2914716601371765, 'learning_rate': 3.638736966726585e-05, 'epoch': 1.04}\n",
            " 38% 70/186 [14:44<23:43, 12.27s/it]05/20/2024 01:23:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.9838, 'learning_rate': 3.4469e-05, 'epoch': 1.12}\n",
            "{'loss': 0.9838, 'grad_norm': 0.3919388949871063, 'learning_rate': 3.44694721402644e-05, 'epoch': 1.12}\n",
            " 40% 75/186 [15:47<22:54, 12.38s/it]05/20/2024 01:24:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.9606, 'learning_rate': 3.2484e-05, 'epoch': 1.20}\n",
            "{'loss': 0.9606, 'grad_norm': 0.32873377203941345, 'learning_rate': 3.2484078074333954e-05, 'epoch': 1.2}\n",
            " 43% 80/186 [16:49<22:24, 12.69s/it]05/20/2024 01:25:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.9339, 'learning_rate': 3.0445e-05, 'epoch': 1.28}\n",
            "{'loss': 0.9339, 'grad_norm': 0.2674310803413391, 'learning_rate': 3.0445338968721287e-05, 'epoch': 1.28}\n",
            " 46% 85/186 [17:53<22:15, 13.22s/it]05/20/2024 01:26:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.9278, 'learning_rate': 2.8368e-05, 'epoch': 1.36}\n",
            "{'loss': 0.9278, 'grad_norm': 0.5990197062492371, 'learning_rate': 2.836778655564653e-05, 'epoch': 1.36}\n",
            " 48% 90/186 [19:06<23:28, 14.67s/it]05/20/2024 01:27:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.9480, 'learning_rate': 2.6266e-05, 'epoch': 1.44}\n",
            "{'loss': 0.948, 'grad_norm': 0.23510292172431946, 'learning_rate': 2.6266229220967818e-05, 'epoch': 1.44}\n",
            " 51% 95/186 [20:08<19:02, 12.56s/it]05/20/2024 01:28:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.9537, 'learning_rate': 2.4156e-05, 'epoch': 1.52}\n",
            "{'loss': 0.9537, 'grad_norm': 0.3118126094341278, 'learning_rate': 2.4155646452913296e-05, 'epoch': 1.52}\n",
            " 54% 100/186 [21:14<18:30, 12.91s/it]05/20/2024 01:29:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.9945, 'learning_rate': 2.2051e-05, 'epoch': 1.60}\n",
            "{'loss': 0.9945, 'grad_norm': 0.45923277735710144, 'learning_rate': 2.2051082071228854e-05, 'epoch': 1.6}\n",
            " 54% 100/186 [21:14<18:30, 12.91s/it][INFO|trainer.py:3305] 2024-05-20 01:29:44,956 >> Saving model checkpoint to saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34/checkpoint-100\n",
            "[INFO|configuration_utils.py:726] 2024-05-20 01:29:45,310 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-20 01:29:45,311 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-05-20 01:29:45,341 >> tokenizer config file saved in saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-05-20 01:29:45,342 >> Special tokens file saved in saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34/checkpoint-100/special_tokens_map.json\n",
            " 56% 105/186 [22:22<17:53, 13.25s/it]05/20/2024 01:30:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.9173, 'learning_rate': 1.9968e-05, 'epoch': 1.68}\n",
            "{'loss': 0.9173, 'grad_norm': 0.3150160610675812, 'learning_rate': 1.9967536997783494e-05, 'epoch': 1.68}\n",
            " 59% 110/186 [23:27<16:42, 13.19s/it]05/20/2024 01:31:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.9593, 'learning_rate': 1.7920e-05, 'epoch': 1.76}\n",
            "{'loss': 0.9593, 'grad_norm': 0.41131919622421265, 'learning_rate': 1.79198623329424e-05, 'epoch': 1.76}\n",
            " 62% 115/186 [24:34<15:37, 13.21s/it]05/20/2024 01:33:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.9773, 'learning_rate': 1.5923e-05, 'epoch': 1.84}\n",
            "{'loss': 0.9773, 'grad_norm': 0.6941399574279785, 'learning_rate': 1.5922653499838137e-05, 'epoch': 1.84}\n",
            " 65% 120/186 [25:37<13:40, 12.44s/it]05/20/2024 01:34:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.0052, 'learning_rate': 1.3990e-05, 'epoch': 1.92}\n",
            "{'loss': 1.0052, 'grad_norm': 0.28430262207984924, 'learning_rate': 1.399014621105914e-05, 'epoch': 1.92}\n",
            " 67% 125/186 [26:29<10:57, 10.78s/it]05/20/2024 01:34:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.9510, 'learning_rate': 1.2136e-05, 'epoch': 2.00}\n",
            "{'loss': 0.951, 'grad_norm': 0.40680909156799316, 'learning_rate': 1.2136114999284288e-05, 'epoch': 2.0}\n",
            " 70% 130/186 [27:38<12:45, 13.67s/it]05/20/2024 01:36:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.0090, 'learning_rate': 1.0374e-05, 'epoch': 2.08}\n",
            "{'loss': 1.009, 'grad_norm': 0.26272958517074585, 'learning_rate': 1.0373775035117305e-05, 'epoch': 2.08}\n",
            " 73% 135/186 [28:46<11:08, 13.11s/it]05/20/2024 01:37:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.9398, 'learning_rate': 8.7157e-06, 'epoch': 2.16}\n",
            "{'loss': 0.9398, 'grad_norm': 0.35337644815444946, 'learning_rate': 8.715687931944449e-06, 'epoch': 2.16}\n",
            " 75% 140/186 [29:48<09:10, 11.97s/it]05/20/2024 01:38:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.9620, 'learning_rate': 7.1737e-06, 'epoch': 2.24}\n",
            "{'loss': 0.962, 'grad_norm': 0.5250584483146667, 'learning_rate': 7.173672209219495e-06, 'epoch': 2.24}\n",
            " 78% 145/186 [30:50<08:23, 12.27s/it]05/20/2024 01:39:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.9878, 'learning_rate': 5.7587e-06, 'epoch': 2.32}\n",
            "{'loss': 0.9878, 'grad_norm': 0.3865847587585449, 'learning_rate': 5.758719052376693e-06, 'epoch': 2.32}\n",
            " 81% 150/186 [31:52<07:19, 12.21s/it]05/20/2024 01:40:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.9974, 'learning_rate': 4.4809e-06, 'epoch': 2.40}\n",
            "{'loss': 0.9974, 'grad_norm': 0.3641302287578583, 'learning_rate': 4.480913969818098e-06, 'epoch': 2.4}\n",
            " 83% 155/186 [33:01<06:54, 13.37s/it]05/20/2024 01:41:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.9408, 'learning_rate': 3.3494e-06, 'epoch': 2.48}\n",
            "{'loss': 0.9408, 'grad_norm': 0.404243528842926, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.48}\n",
            " 86% 160/186 [33:59<05:19, 12.30s/it]05/20/2024 01:42:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.9057, 'learning_rate': 2.3721e-06, 'epoch': 2.56}\n",
            "{'loss': 0.9057, 'grad_norm': 0.2966192662715912, 'learning_rate': 2.372137318741968e-06, 'epoch': 2.56}\n",
            " 89% 165/186 [35:06<04:27, 12.74s/it]05/20/2024 01:43:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.8768, 'learning_rate': 1.5562e-06, 'epoch': 2.64}\n",
            "{'loss': 0.8768, 'grad_norm': 0.5003758668899536, 'learning_rate': 1.5561966963229924e-06, 'epoch': 2.64}\n",
            " 91% 170/186 [36:07<03:17, 12.34s/it]05/20/2024 01:44:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.0056, 'learning_rate': 9.0736e-07, 'epoch': 2.72}\n",
            "{'loss': 1.0056, 'grad_norm': 0.3800210952758789, 'learning_rate': 9.073589027514789e-07, 'epoch': 2.72}\n",
            " 94% 175/186 [37:19<02:40, 14.55s/it]05/20/2024 01:45:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.8955, 'learning_rate': 4.3025e-07, 'epoch': 2.80}\n",
            "{'loss': 0.8955, 'grad_norm': 0.23149621486663818, 'learning_rate': 4.302487264785521e-07, 'epoch': 2.8}\n",
            " 97% 180/186 [38:24<01:15, 12.65s/it]05/20/2024 01:46:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.9306, 'learning_rate': 1.2827e-07, 'epoch': 2.88}\n",
            "{'loss': 0.9306, 'grad_norm': 0.3504321575164795, 'learning_rate': 1.2826691520262114e-07, 'epoch': 2.88}\n",
            " 99% 185/186 [39:26<00:12, 12.46s/it]05/20/2024 01:47:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.8853, 'learning_rate': 3.5659e-09, 'epoch': 2.96}\n",
            "{'loss': 0.8853, 'grad_norm': 0.3388601839542389, 'learning_rate': 3.565936007254855e-09, 'epoch': 2.96}\n",
            "100% 186/186 [39:38<00:00, 12.35s/it][INFO|trainer.py:2316] 2024-05-20 01:48:08,810 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2378.7692, 'train_samples_per_second': 1.261, 'train_steps_per_second': 0.078, 'train_loss': 0.9711038970178173, 'epoch': 2.98}\n",
            "100% 186/186 [39:38<00:00, 12.79s/it]\n",
            "[INFO|trainer.py:3305] 2024-05-20 01:48:08,812 >> Saving model checkpoint to saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:726] 2024-05-20 01:48:09,133 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/8f5f3a02ec472594e949c39f8e38c7be8d983bcd/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-20 01:48:09,134 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-05-20 01:48:09,160 >> tokenizer config file saved in saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-05-20 01:48:09,160 >> Special tokens file saved in saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.976\n",
            "  total_flos               = 25073202GF\n",
            "  train_loss               =     0.9711\n",
            "  train_runtime            = 0:39:38.76\n",
            "  train_samples_per_second =      1.261\n",
            "  train_steps_per_second   =      0.078\n",
            "Figure saved at: saves/Phi3-3.8B-4k-Chat/lora/train_2024-05-20-01-03-34/training_loss.png\n",
            "05/20/2024 01:48:09 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "[INFO|modelcard.py:450] 2024-05-20 01:48:09,383 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2619, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/cli.py\", line 69, in main\n",
            "    run_web_ui()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/webui/interface.py\", line 76, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2524, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2623, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://c3eb4ce15ca1332ac2.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install poetry\n"
      ],
      "metadata": {
        "id": "o6_t4qH1s2pt",
        "outputId": "33a5c5c8-4ad7-44c2-e870-7bc4b1423695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting poetry\n",
            "  Downloading poetry-1.8.3-py3-none-any.whl (249 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/249.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m204.8/249.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from poetry) (1.2.1)\n",
            "Requirement already satisfied: cachecontrol[filecache]<0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (0.14.0)\n",
            "Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n",
            "  Downloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crashtest<0.5.0,>=0.4.1 (from poetry)\n",
            "  Downloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting dulwich<0.22.0,>=0.21.2 (from poetry)\n",
            "  Downloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.7/514.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (2.19.1)\n",
            "Collecting installer<0.8.0,>=0.7.0 (from poetry)\n",
            "  Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keyring<25.0.0,>=24.0.0 (from poetry)\n",
            "  Downloading keyring-24.3.1-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from poetry) (24.0)\n",
            "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (4.9.0)\n",
            "Collecting pkginfo<2.0,>=1.10 (from poetry)\n",
            "  Downloading pkginfo-1.10.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (4.2.1)\n",
            "Collecting poetry-core==1.9.0 (from poetry)\n",
            "  Downloading poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting poetry-plugin-export<2.0.0,>=1.6.0 (from poetry)\n",
            "  Downloading poetry_plugin_export-1.8.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry) (1.1.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.26 in /usr/local/lib/python3.10/dist-packages (from poetry) (2.31.0)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from poetry)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.10/dist-packages (from poetry) (1.5.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from poetry) (2.0.1)\n",
            "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /usr/local/lib/python3.10/dist-packages (from poetry) (0.12.0)\n",
            "Collecting trove-classifiers>=2022.5.19 (from poetry)\n",
            "  Downloading trove_classifiers-2024.5.17-py3-none-any.whl (13 kB)\n",
            "Collecting virtualenv<21.0.0,>=20.23.0 (from poetry)\n",
            "  Downloading virtualenv-20.26.2-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (1.0.8)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (3.14.0)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n",
            "  Downloading rapidfuzz-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from dulwich<0.22.0,>=0.21.2->poetry) (2.0.7)\n",
            "Collecting jaraco.classes (from keyring<25.0.0,>=24.0.0->poetry)\n",
            "  Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.4 in /usr/local/lib/python3.10/dist-packages (from keyring<25.0.0,>=24.0.0->poetry) (7.1.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.1)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry) (0.7.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.26->poetry) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.26->poetry) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.26->poetry) (2024.2.2)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.23.0->poetry)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry) (3.18.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry) (10.1.0)\n",
            "Installing collected packages: trove-classifiers, distlib, virtualenv, rapidfuzz, poetry-core, pkginfo, jaraco.classes, installer, dulwich, crashtest, requests-toolbelt, keyring, cleo, poetry-plugin-export, poetry\n",
            "  Attempting uninstall: keyring\n",
            "    Found existing installation: keyring 23.5.0\n",
            "    Uninstalling keyring-23.5.0:\n",
            "      Successfully uninstalled keyring-23.5.0\n",
            "Successfully installed cleo-2.1.0 crashtest-0.4.1 distlib-0.3.8 dulwich-0.21.7 installer-0.7.0 jaraco.classes-3.4.0 keyring-24.3.1 pkginfo-1.10.0 poetry-1.8.3 poetry-core-1.9.0 poetry-plugin-export-1.8.0 rapidfuzz-3.9.1 requests-toolbelt-1.0.0 trove-classifiers-2024.5.17 virtualenv-20.26.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                        # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "  template=\"llama3\",                     # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,               # the batch size\n",
        "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "  logging_steps=10,                      # log every 10 steps\n",
        "  warmup_ratio=0.1,                      # use warmup scheduler\n",
        "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                     # the learning rate\n",
        "  num_train_epochs=3.0,                    # the epochs of training\n",
        "  max_samples=500,                      # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "  quantization_bit=4,                     # use 4-bit QLoRA\n",
        "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "  use_unsloth=True,                      # use UnslothAI's LoRA optimization for 2x faster training\n",
        "  fp16=True,                         # use float16 mixed precision training\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS0Qk5OR0i4Q",
        "outputId": "cc622c42-b628-4be4-fe45-9b9b5dad1247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1372, in _path_importer_cache\n",
            "KeyError: '/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 5, in <module>\n",
            "    from llamafactory.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/__init__.py\", line 3, in <module>\n",
            "    from .cli import VERSION\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/cli.py\", line 4, in <module>\n",
            "    from .api.app import run_api\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/api/app.py\", line 7, in <module>\n",
            "    from ..chat import ChatModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/chat/__init__.py\", line 2, in <module>\n",
            "    from .chat_model import ChatModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/chat/chat_model.py\", line 7, in <module>\n",
            "    from .hf_engine import HuggingfaceEngine\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/chat/hf_engine.py\", line 10, in <module>\n",
            "    from ..data import get_template_and_fix_tokenizer\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/data/__init__.py\", line 1, in <module>\n",
            "    from .collator import KTODataCollatorWithPadding, PairwiseDataCollatorWithPadding\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llamafactory/data/collator.py\", line 5, in <module>\n",
            "    from transformers import DataCollatorForSeq2Seq\n",
            "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1500, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1510, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/data/__init__.py\", line 26, in <module>\n",
            "    from .metrics import glue_compute_metrics, xnli_compute_metrics\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/data/metrics/__init__.py\", line 19, in <module>\n",
            "    from scipy.stats import pearsonr, spearmanr\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\", line 608, in <module>\n",
            "    from ._stats_py import *\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\", line 39, in <module>\n",
            "    from scipy.spatial.distance import cdist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/spatial/__init__.py\", line 110, in <module>\n",
            "    from ._kdtree import *\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/spatial/_kdtree.py\", line 4, in <module>\n",
            "    from ._ckdtree import cKDTree, cKDTreeNode\n",
            "  File \"_ckdtree.pyx\", line 12, in init scipy.spatial._ckdtree\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/__init__.py\", line 287, in <module>\n",
            "    from . import csgraph\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/__init__.py\", line 185, in <module>\n",
            "    from ._laplacian import laplacian\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/_laplacian.py\", line 7, in <module>\n",
            "    from scipy.sparse.linalg import LinearOperator\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/__init__.py\", line 120, in <module>\n",
            "    from ._isolve import *\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1408, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1374, in _path_importer_cache\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1350, in _path_hooks\n",
            "  File \"<frozen zipimport>\", line 76, in __init__\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 147, in _path_stat\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        "  use_unsloth=True,                     # use UnslothAI's LoRA optimization for 2x faster generation\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93a8a7d-6d7a-4b71-9512-a2df827bbe0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-18 14:30:42,715 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-18 14:30:42,716 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-18 14:30:42,718 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2087] 2024-05-18 14:30:42,719 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-18 14:30:43,133 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:43 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.data.template:Replace eos token: <|eot_id|>\n",
            "[INFO|configuration_utils.py:726] 2024-05-18 14:30:43,246 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-18 14:30:43,249 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:43 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.utils.quantization:Loading ?-bit BITSANDBYTES-quantized model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:43 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.patcher:Using KV cache for faster generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:43 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.adapter:Upcasting trainable params to float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:43 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.adapter:Fine-tuning method: LoRA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:726] 2024-05-18 14:30:43,708 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-18 14:30:43,710 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:726] 2024-05-18 14:30:43,814 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-18 14:30:43,816 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:726] 2024-05-18 14:30:43,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-05-18 14:30:43,921 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3429] 2024-05-18 14:30:44,045 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
            "[INFO|modeling_utils.py:1494] 2024-05-18 14:30:44,089 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:928] 2024-05-18 14:30:44,095 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4170] 2024-05-18 14:30:56,748 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4178] 2024-05-18 14:30:56,750 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:883] 2024-05-18 14:30:56,863 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
            "[INFO|configuration_utils.py:928] 2024-05-18 14:30:56,865 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ]\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,143 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,144 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,146 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,149 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,156 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,160 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,161 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-05-18 14:30:57,163 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-05-18 14:30:57,573 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[WARNING|logging.py:329] 2024-05-18 14:30:59,446 >> Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:59 - INFO - llamafactory.model.adapter - Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.adapter:Loaded adapter(s): llama3_lora\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/18/2024 14:30:59 - INFO - llamafactory.model.loader - all params: 8051232768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llamafactory.model.loader:all params: 8051232768\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
            "\n",
            "User: hello\n",
            "Assistant: Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?\n",
            "\n",
            "User: give me three tips for keep fit\n",
            "Assistant: 1. Start small: Don't try to change everything at once. Start with short, manageable workouts and gradually increase the intensity and duration over time.\n",
            "\n",
            "2. Find an exercise you enjoy: It's easier to stick to a workout routine if you enjoy the activity. Experiment with different types of exercise, such as running, swimming, yoga, or weightlifting, to find what works best for you.\n",
            "\n",
            "3. Make it a habit: Consistency is key when it comes to keeping fit. Try to schedule your workouts at the same time every day, so it becomes a habit. Also, find a workout buddy or join a fitness class to help keep you motivated and accountable.\n",
            "\n",
            "User: thank you\n",
            "Assistant: You're welcome! I hope the tips help you in your fitness journey.\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mcNcHcA4bf4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "  export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "  export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}